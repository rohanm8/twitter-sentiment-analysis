{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3e47a0a-9ed2-4249-9226-85d94b6f1d0b",
   "metadata": {},
   "source": [
    "# Twitter Sentiment Analysis\n",
    "## Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cef387-6ec2-4745-a0fb-0abc265137ac",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d2d4b45-70c2-4547-bd12-c814f92a7304",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81f51d4-2c45-477d-9dc9-d91a83620fa7",
   "metadata": {},
   "source": [
    "### Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "373863e7-2b55-49e2-b76a-a21c10da5661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape of training dataset: (74682, 4)\n",
      "Original shape of validation dataset: (1000, 4)\n"
     ]
    }
   ],
   "source": [
    "# Save dataset paths into variables\n",
    "train_df_path = \"datasets/twitter_training.csv\"\n",
    "val_df_path = \"datasets/twitter_validation.csv\"\n",
    "\n",
    "# Define column names and load datasets from path variables\n",
    "columns = ['Tweet_ID', 'Entity', 'Sentiment', 'Tweet_Content'] \n",
    "train_df = pd.read_csv(train_df_path, header=None, names=columns)\n",
    "val_df = pd.read_csv(val_df_path, header=None, names=columns)\n",
    "\n",
    "# Print original dataset shapes\n",
    "print(f\"Original shape of training dataset: {train_df.shape}\")\n",
    "print(f\"Original shape of validation dataset: {val_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835445dc-7758-4e7c-9be0-cb4155a42938",
   "metadata": {},
   "source": [
    "### Clean and Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "818fd6b0-3679-49f6-9e1d-6a5261270f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset - Drop NULL and Merge: (73996, 4)\n",
      "Validation dataset - Drop NULL and Merge: (1000, 4)\n"
     ]
    }
   ],
   "source": [
    "# Drop rows in training dataset that do not have content in 'Tweet_Content'\n",
    "train_df = train_df.dropna(subset=['Tweet_Content'])\n",
    "val_df = val_df.dropna(subset=['Tweet_Content'])\n",
    "\n",
    "# Convert 'Irrelevant' sentiments to neutral\n",
    "train_df['Sentiment'] = train_df['Sentiment'].replace('Irrelevant', 'Neutral')\n",
    "val_df['Sentiment'] = val_df['Sentiment'].replace('Irrelevant', 'Neutral')\n",
    "\n",
    "# Print dataset shapes after dropping null rows and merging 'Irrelevant' with 'Neutral'\n",
    "print(f\"Training dataset - Drop NULL and Merge: {train_df.shape}\")\n",
    "print(f\"Validation dataset - Drop NULL and Merge: {val_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fae1d082-f8bb-48c9-b9eb-16918362b6bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset - Drop duplicates: (70887, 4)\n",
      "Validation dataset - Drop duplicates: (1000, 4)\n"
     ]
    }
   ],
   "source": [
    "# Remove duplicate rows from training dataset\n",
    "train_df = train_df.drop_duplicates(subset=['Tweet_Content', 'Entity', 'Sentiment'])\n",
    "val_df = val_df.drop_duplicates(subset=['Tweet_Content', 'Entity', 'Sentiment'])\n",
    "\n",
    "print(f\"Training dataset - Drop duplicates: {train_df.shape}\")\n",
    "print(f\"Validation dataset - Drop duplicates: {val_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c5fd90-6f9f-4c1e-9fd3-4a07176910af",
   "metadata": {},
   "source": [
    "### Setup SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7dfcb81-9805-4ac9-8d92-f1a51c7d4ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spacy and disable unwanted components\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5a242d5-7899-44f2-8b2d-438dba0c2a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_spacy(doc):\n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        if (not token.is_stop or token.text.lower() in ['no', 'not', 'never', 'neither', 'nor']) \\\n",
    "            and not token.is_punct \\\n",
    "            and not token.like_url \\\n",
    "            and not token.like_email \\\n",
    "            and not token.is_space:\n",
    "                tokens.append(token.lemma_.lower())\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c38a5017-7909-490b-970d-f61e4770abaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(df, text_col):\n",
    "    \"\"\"Helper to process a dataframe in batches\"\"\"\n",
    "    clean_texts = []\n",
    "    for doc in nlp.pipe(df[text_col].astype(str).tolist(), batch_size=1000):\n",
    "        clean_texts.append(clean_text_spacy(doc))\n",
    "    return clean_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "466618bf-38e5-4c73-b521-c7f27b42b38f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning training data...\n",
      "Cleaning validation data...\n"
     ]
    }
   ],
   "source": [
    "print(\"Cleaning training data...\")\n",
    "train_df['clean_text'] = process_batch(train_df, 'Tweet_Content')\n",
    "\n",
    "print(\"Cleaning validation data...\")\n",
    "val_df['clean_text'] = process_batch(val_df, 'Tweet_Content')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adda491-e3cf-4312-a11b-ec7dac785385",
   "metadata": {},
   "source": [
    "#### Label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1de650fb-0768-4df1-b8de-86035ad5e149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Mapping: {'Negative': np.int64(0), 'Neutral': np.int64(1), 'Positive': np.int64(2)}\n"
     ]
    }
   ],
   "source": [
    "# Convert 'Positive', 'Negative', 'Neutral' to 0, 1, 2\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Fit on training data to learn the mapping\n",
    "train_df['label_encoded'] = le.fit_transform(train_df['Sentiment'])\n",
    "\n",
    "# Transform validation data using same mapping\n",
    "val_df['label_encoded'] = le.transform(val_df['Sentiment'])\n",
    "\n",
    "print(f\"Label Mapping: {dict(zip(le.classes_, le.transform(le.classes_)))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e573f5b0-922b-499a-8ab8-84ac2fcd06dc",
   "metadata": {},
   "source": [
    "### Split training data into Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dbdf9916-e2ad-4a2b-abc3-4fbe511f7bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete. Files saved.\n",
      "\n",
      "train_preprocessed.csv = (56709, 6)\n",
      "test_preprocessed.csv = (14178, 6)\n",
      "validation_preprocessed.csv = (1000, 6)\n"
     ]
    }
   ],
   "source": [
    "train_split, test_split = train_test_split(\n",
    "    train_df,\n",
    "    test_size = 0.2,\n",
    "    random_state = 42,\n",
    "    stratify = train_df['Sentiment']\n",
    ")\n",
    "\n",
    "# Save preprocessed CSV\n",
    "\n",
    "train_split.to_csv('datasets/train_preprocessed.csv', index=False)\n",
    "test_split.to_csv('datasets/test_preprocessed.csv', index=False)\n",
    "val_df.to_csv('datasets/validation_preprocessed.csv', index=False)\n",
    "\n",
    "print(\"Processing complete. Files saved.\\n\")\n",
    "print(f\"train_preprocessed.csv = {train_split.shape}\")\n",
    "print(f\"test_preprocessed.csv = {test_split.shape}\")\n",
    "print(f\"validation_preprocessed.csv = {val_df.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f721fa4-53c7-42d7-ab61-b2c150747498",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
